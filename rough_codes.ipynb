{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e77b00de-6070-4a7c-b460-eafe6b90cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ============== A-to-Z Project Script (FINAL OPTIMIZED VERSION) ==============\n",
    "# Project: Meta-Learning the Latent Manifold with Learnable-Interaction Neurons\n",
    "# Course: Neural Networks\n",
    "# Due Date: September 14th, 2025\n",
    "#\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "import persim\n",
    "import scipy.linalg\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "from scipy.stats import entropy\n",
    "import warnings\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.utils.deprecation\")\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # For better error messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf93dd5-7ef2-41f9-b6e3-3acca1672a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================\n",
    "# == 1. MODEL DEFINITIONS\n",
    "# ==========================================================================================\n",
    "\n",
    "class ChebyshevLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, order=3):\n",
    "        super().__init__()\n",
    "        self.in_features, self.out_features, self.order = in_features, out_features, order\n",
    "        self.coeffs = nn.Parameter(torch.empty(out_features, in_features, order + 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        with torch.no_grad():\n",
    "            self.coeffs[:, :, 0].uniform_(-1e-4, 1e-4)\n",
    "            self.coeffs[:, :, 2:].uniform_(-1e-4, 1e-4)\n",
    "            if self.order >= 1:\n",
    "                t1_coeffs = torch.empty(self.out_features, self.in_features)\n",
    "                nn.init.xavier_uniform_(t1_coeffs)\n",
    "                self.coeffs.data[:, :, 1] = t1_coeffs\n",
    "\n",
    "    def forward(self, x):\n",
    "        cheby_poly_list = []\n",
    "        t0 = torch.ones_like(x)\n",
    "        cheby_poly_list.append(t0)\n",
    "        if self.order > 0:\n",
    "            t1 = x\n",
    "            cheby_poly_list.append(t1)\n",
    "        for k in range(2, self.order + 1):\n",
    "            tk = 2 * x * cheby_poly_list[k - 1] - cheby_poly_list[k - 2]\n",
    "            cheby_poly_list.append(tk)\n",
    "        cheby_polys = torch.stack(cheby_poly_list, dim=2)\n",
    "        adaptive_weights = torch.einsum('oik,bik->boi', self.coeffs, cheby_polys)\n",
    "        output = torch.einsum('bi,boi->bo', x, adaptive_weights)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0941340c-2e3e-4858-89c9-6e32922bf8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineVAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU())\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.decoder = nn.Sequential(nn.Linear(latent_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, input_dim), nn.Sigmoid())\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x.view(-1, 784))\n",
    "        return self.fc_mu(h), self.fc_log_var(h)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d792675-3033-4cb9-8a6c-50bfb8a12e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChebyshevVAE(BaselineVAE):\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20, cheby_order=3):\n",
    "        super().__init__(input_dim, hidden_dim, latent_dim)\n",
    "        self.encoder_cheby = ChebyshevLayer(input_dim, hidden_dim, order=cheby_order)\n",
    "        self.fc_mu = ChebyshevLayer(hidden_dim, latent_dim, order=cheby_order)\n",
    "        self.fc_log_var = ChebyshevLayer(hidden_dim, latent_dim, order=cheby_order)\n",
    "        self.decoder_cheby1 = ChebyshevLayer(latent_dim, hidden_dim, order=cheby_order)\n",
    "        self.decoder_final = nn.Sequential(nn.Linear(hidden_dim, input_dim), nn.Sigmoid())\n",
    "\n",
    "    def encode(self, x):\n",
    "        x_flat = x.view(-1, 784)\n",
    "        model_input = (x_flat * 2) -1\n",
    "        h = torch.tanh(self.encoder_cheby(model_input))\n",
    "        return self.fc_mu(h), self.fc_log_var(h)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = torch.tanh(self.decoder_cheby1(z))\n",
    "        return self.decoder_final(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a78b04-a24f-4785-87ef-938388e4c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvChebyshevVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32, cheby_order=5, img_channels=1):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.cheby_order = cheby_order\n",
    "\n",
    "        # Deeper Encoder: 4 Conv layers with BatchNorm and LeakyReLU\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(img_channels, 32, kernel_size=3, stride=1, padding=1),  # 28x28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),  # 14x14\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 14x14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),  # 7x7\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Block 3 (shallower to avoid over-compression)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # 7x7\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        # Flattened: 128 * 7 * 7 = 6272\n",
    "        flat_dim = 128 * 7 * 7\n",
    "        self.encoder_cheby = ChebyshevLayer(flat_dim, 512, order=cheby_order)  # Increased hidden\n",
    "        self.fc_mu = nn.Linear(512, latent_dim)  # Linear for mu/logvar (stable)\n",
    "        self.fc_log_var = nn.Linear(512, latent_dim)\n",
    "\n",
    "        # Decoder: Symmetric, with residuals + Chebyshev\n",
    "        self.decoder_cheby = ChebyshevLayer(latent_dim, 512, order=cheby_order)\n",
    "        self.decoder_fc = nn.Linear(512, flat_dim)  # Expand before reshape\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            # Reshape to 128x7x7 after fc\n",
    "            # Block 3 (reverse)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),  # 7x7 -> 7x7\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # 14x14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Residual skip: Add from encoder if needed (simplified here)\n",
    "            # Block 2 (reverse)\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),  # 14x14\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # 28x28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Block 1 (reverse)\n",
    "            nn.ConvTranspose2d(32, img_channels, kernel_size=3, stride=1, padding=1),  # 28x28\n",
    "            nn.Sigmoid()  # Output [0,1]\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x: (B, 1, 28, 28)\n",
    "        h_conv = self.encoder_conv(x)\n",
    "        h_flat = h_conv.view(h_conv.size(0), -1)  # (B, 6272)\n",
    "        \n",
    "        # Scale to [-1,1] more robustly\n",
    "        h_norm = torch.tanh(h_flat)  # Bounded normalization\n",
    "        h_cheby = self.encoder_cheby(h_norm)\n",
    "        \n",
    "        mu = self.fc_mu(h_cheby)\n",
    "        log_var = self.fc_log_var(h_cheby)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h_cheby = self.decoder_cheby(z)\n",
    "        h_expanded = self.decoder_fc(h_cheby).view(-1, 128, 7, 7)  # Reshape\n",
    "        return self.decoder_conv(h_expanded)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f030ba0-4553-447d-aa5c-c171879be66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (All loss functions, data loaders, evaluation, and visualization functions remain the same)\n",
    "# ...\n",
    "def vae_loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51828ea-1d2e-4bf1-9705-c3c2aac408f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_loss(X_batch, Z_batch):\n",
    "    X_np = X_batch.view(X_batch.shape[0], -1).detach().cpu().numpy()\n",
    "    Z_np = Z_batch.detach().cpu().numpy()\n",
    "    X_point_cloud, Z_point_cloud = X_np[None, :, :], Z_np[None, :, :]\n",
    "    vrp = VietorisRipsPersistence(homology_dimensions=[0, 1])\n",
    "    X_diag = vrp.fit_transform(X_point_cloud)[0]\n",
    "    Z_diag = vrp.fit_transform(Z_point_cloud)[0]\n",
    "    total_distance = 0.0\n",
    "    for dim in [0, 1]:\n",
    "        X_diag_dim = X_diag[X_diag[:, 2] == dim][:, :2]\n",
    "        Z_diag_dim = Z_diag[Z_diag[:, 2] == dim][:, :2]\n",
    "        if X_diag_dim.shape[0] == 0 and Z_diag_dim.shape[0] == 0:\n",
    "            distance_dim = 0.0\n",
    "        else:\n",
    "            distance_dim = persim.wasserstein(X_diag_dim, Z_diag_dim)\n",
    "        total_distance += distance_dim\n",
    "    return torch.tensor(total_distance, device=X_batch.device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3543ffc-539c-4f97-8ee1-8a4dbf520566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disentanglement_loss(z_shared1, z_distinct1, z_shared2, z_distinct2):\n",
    "    loss_shared = 1 - nn.functional.cosine_similarity(z_shared1, z_shared2, dim=-1).mean()\n",
    "    pdist = nn.PairwiseDistance(p=2)\n",
    "    loss_distinct = -pdist(z_distinct1, z_distinct2).mean()\n",
    "    return loss_shared + loss_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b38bdda-6bce-4e80-83c9-988c430f7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedTransform:\n",
    "    def __init__(self, transform): self.transform = transform\n",
    "    def __call__(self, x): return self.transform(x), self.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f422ab-990b-47d8-a50c-80826d95f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size, use_paired_transforms=False):\n",
    "    base_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    if use_paired_transforms:\n",
    "        train_transform = PairedTransform(transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomRotation(10), transforms.ToTensor()]))\n",
    "        train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=train_transform)\n",
    "    else:\n",
    "        train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=base_transform)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=base_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd39ebfb-c376-4298-9452-1c8f8d61ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(model, test_loader, device, latent_dim, num_samples=5000):\n",
    "    print(\"Calculating FID score...\")\n",
    "    weights = Inception_V3_Weights.DEFAULT\n",
    "    inception_model = inception_v3(weights=weights, transform_input=False).to(device)\n",
    "    inception_model.fc = nn.Identity()\n",
    "    inception_model.eval()\n",
    "    \n",
    "    real_features = []\n",
    "    bs_test = test_loader.batch_size\n",
    "    for data, _ in test_loader:\n",
    "        if len(real_features) * bs_test >= num_samples: break\n",
    "        data = data.to(device)\n",
    "        data_resized = nn.functional.interpolate(data, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        data_rgb = data_resized.repeat(1, 3, 1, 1)\n",
    "        with torch.no_grad():\n",
    "            features = inception_model(data_rgb)\n",
    "        real_features.append(features.cpu().numpy())\n",
    "    real_features = np.concatenate(real_features, axis=0)[:num_samples]\n",
    "\n",
    "    generated_features = []\n",
    "    bs_gen = 100\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, num_samples, bs_gen):\n",
    "            z = torch.randn(bs_gen, latent_dim).to(device)\n",
    "            samples = model.decode(z).view(-1, 1, 28, 28)\n",
    "            samples_resized = nn.functional.interpolate(samples, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            samples_rgb = samples_resized.repeat(1, 3, 1, 1)\n",
    "            features = inception_model(samples_rgb)\n",
    "            generated_features.append(features.cpu().numpy())\n",
    "    generated_features = np.concatenate(generated_features, axis=0)\n",
    "\n",
    "    mu1, sigma1 = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = np.mean(generated_features, axis=0), np.cov(generated_features, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    covmean = scipy.linalg.sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean): covmean = covmean.real\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830fa746-9da0-4953-80fc-d1301c92dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inception_score(model, device, latent_dim, num_samples=5000, splits=10):\n",
    "    print(\"Calculating Inception Score (IS)...\")\n",
    "    weights = Inception_V3_Weights.DEFAULT\n",
    "    inception_model = inception_v3(weights=weights, transform_input=False).to(device)\n",
    "    inception_model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    bs = 100\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, num_samples, bs):\n",
    "            z = torch.randn(bs, latent_dim).to(device)\n",
    "            samples = model.decode(z).view(-1, 1, 28, 28)\n",
    "            samples_resized = nn.functional.interpolate(samples, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            samples_rgb = samples_resized.repeat(1, 3, 1, 1)\n",
    "            preds = inception_model(samples_rgb)\n",
    "            all_preds.append(nn.functional.softmax(preds, dim=1).cpu().numpy())\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "    scores = []\n",
    "    for i in range(splits):\n",
    "        part = all_preds[i * (num_samples // splits): (i + 1) * (num_samples // splits), :]\n",
    "        kl_divs = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, axis=0), 0)))\n",
    "        kl_divs = np.mean(np.sum(kl_divs, axis=1))\n",
    "        scores.append(np.exp(kl_divs))\n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8461e09c-78dd-4cc2-9703-41769ad9dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructions(model, test_loader, device, save_path):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data, _ = next(iter(test_loader))\n",
    "        data = data.to(device)\n",
    "        recon, _, _ = model(data)\n",
    "        comparison = torch.cat([data.view(-1, 1, 28, 28)[:8], recon.view(-1, 1, 28, 28)[:8]])\n",
    "        from torchvision.utils import save_image\n",
    "        save_image(comparison.cpu(), save_path, nrow=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fc97eb6-bae9-4ed9-acdf-05f84b56a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_samples(model, device, save_path, num_samples=64):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, model.latent_dim).to(device)\n",
    "        samples = model.decode(z).cpu()\n",
    "        from torchvision.utils import save_image\n",
    "        save_image(samples.view(num_samples, 1, 28, 28), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "794c04e5-e60b-481a-8871-79d5796ce1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_latent_space(model, test_loader, device, save_path):\n",
    "    model.eval()\n",
    "    all_z, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.to(device)\n",
    "            mu, _ = model.encode(data)\n",
    "            all_z.append(mu.cpu().numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "    all_z = np.concatenate(all_z, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    reducer = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "    embedding = reducer.fit_transform(all_z)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], c=all_labels, cmap='Spectral', s=5)\n",
    "    plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "    plt.title('UMAP Projection of the Latent Space')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733434c9-eb7c-47b1-8a6d-74a3f6244f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================\n",
    "# == MAIN TRAINING & EVALUATION SCRIPT\n",
    "# ==========================================================================================\n",
    "def main(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    results_dir = os.path.join(\"opti_results_new\", args.run_name)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    use_paired = args.model_type in ['B', 'D', 'E'] # Include new model type\n",
    "    train_loader, test_loader = get_dataloaders(args.batch_size, use_paired_transforms=use_paired)\n",
    "\n",
    "    if args.model_type in ['A', 'B']:\n",
    "        model = BaselineVAE(latent_dim=args.latent_dim).to(device)\n",
    "    elif args.model_type in ['C', 'D']:\n",
    "         model = ChebyshevVAE(latent_dim=args.latent_dim, cheby_order=args.cheby_order).to(device)\n",
    "    elif args.model_type == 'E':\n",
    "        model = ConvChebyshevVAE(latent_dim=args.latent_dim, cheby_order=args.cheby_order).to(device)\n",
    "    print(f\"Initialized Model {args.model_type}\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "    \n",
    "    # ... (Training loop is modified to handle the new ConvChebyshevVAE data flow)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            current_gamma = args.gamma if epoch > args.warmup_epochs else 0.0\n",
    "            current_delta = args.delta if epoch > args.warmup_epochs else 0.0\n",
    "\n",
    "            if use_paired:\n",
    "                (data1, data2), _ = data\n",
    "                data1, data2 = data1.to(device), data2.to(device)\n",
    "                recon, mu, log_var = model(data1)\n",
    "                loss_v = vae_loss_function(recon, data1, mu, log_var)\n",
    "                \n",
    "                loss_t = torch.tensor(0.0, device=device)\n",
    "                if current_gamma > 0 and batch_idx % 20 == 0:\n",
    "                    z = model.reparameterize(mu, log_var)\n",
    "                    loss_t = topological_loss(data1, z)\n",
    "                \n",
    "                loss_d = torch.tensor(0.0, device=device)\n",
    "                if current_delta > 0:\n",
    "                    mu1, _ = model.encode(data1)\n",
    "                    mu2, _ = model.encode(data2)\n",
    "                    z_shared1, z_distinct1 = torch.chunk(mu1, 2, dim=1)\n",
    "                    z_shared2, z_distinct2 = torch.chunk(mu2, 2, dim=1)\n",
    "                    loss_d = disentanglement_loss(z_shared1, z_distinct1, z_shared2, z_distinct2)\n",
    "                \n",
    "                loss = loss_v + current_gamma * loss_t + current_delta * loss_d\n",
    "            else:\n",
    "                data, _ = data\n",
    "                data = data.to(device)\n",
    "                recon, mu, log_var = model(data)\n",
    "                loss = vae_loss_function(recon, data, mu, log_var)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.grad_clip)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, _ in test_loader:\n",
    "                data = data.to(device)\n",
    "                recon, mu, log_var = model(data)\n",
    "                total_test_loss += vae_loss_function(recon, data, mu, log_var).item()\n",
    "        avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        print(f'====> Epoch: {epoch} Average train loss: {avg_train_loss:.4f} | Average test loss: {avg_test_loss:.4f}')\n",
    "    \n",
    "    # ... (Rest of the main function for saving and evaluation)\n",
    "    print(\"Training finished.\")\n",
    "    torch.save(model.state_dict(), os.path.join(results_dir, \"model.pth\"))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_dir, 'loss_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    save_reconstructions(model, test_loader, device, os.path.join(results_dir, 'reconstructions.png'))\n",
    "    save_generated_samples(model, device, os.path.join(results_dir, 'generated_samples.png'))\n",
    "    save_latent_space(model, test_loader, device, os.path.join(results_dir, 'latent_space.png'))\n",
    "\n",
    "    print(f\"Results saved to {results_dir}\")\n",
    "\n",
    "    fid_score = calculate_fid(model, test_loader, device, args.latent_dim)\n",
    "    is_mean, is_std = calculate_inception_score(model, device, args.latent_dim)\n",
    "    print(f\"====> Final FID Score: {fid_score:.4f}\")\n",
    "    print(f\"====> Final Inception Score: {is_mean:.4f} ± {is_std:.4f}\")\n",
    "    with open(os.path.join(results_dir, \"final_metrics.txt\"), \"w\") as f:\n",
    "        f.write(f\"Final Test Loss: {avg_test_loss}\\n\")\n",
    "        f.write(f\"Final FID Score: {fid_score}\\n\")\n",
    "        f.write(f\"Final Inception Score: {is_mean} ± {is_std}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e3e18-964d-49b2-bf41-9b24af6eb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    class Args:\n",
    "        # --- OPTIMIZED MODEL CONFIGURATION ---\n",
    "        model_type = 'A' # Run the new ConvChebyshevVAE\n",
    "        batch_size = 64\n",
    "        epochs = 20\n",
    "        lr = 1e-4 # CNNs can handle a slightly higher learning rate\n",
    "        latent_dim = 20\n",
    "        cheby_order = 3\n",
    "        gamma = 0.05\n",
    "        delta = 0.5\n",
    "        grad_clip = 1.0 # CNNs are generally more stable\n",
    "        warmup_epochs = 5\n",
    "        run_name = 'ModelA_Conv_Optimized'\n",
    "\n",
    "    args = Args()\n",
    "    main(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Args:\n",
    "        # --- OPTIMIZED MODEL CONFIGURATION ---\n",
    "        model_type = 'B' # Run the new ConvChebyshevVAE\n",
    "        batch_size = 64\n",
    "        epochs = 20\n",
    "        lr = 1e-4 # CNNs can handle a slightly higher learning rate\n",
    "        latent_dim = 20\n",
    "        cheby_order = 3\n",
    "        gamma = 0.05\n",
    "        delta = 0.5\n",
    "        grad_clip = 1.0 # CNNs are generally more stable\n",
    "        warmup_epochs = 5\n",
    "        run_name = 'ModelB_Conv_Optimized'\n",
    "\n",
    "    args = Args()\n",
    "    main(args)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Args:\n",
    "        # --- OPTIMIZED MODEL CONFIGURATION ---\n",
    "        model_type = 'C' # Run the new ConvChebyshevVAE\n",
    "        batch_size = 64\n",
    "        epochs = 20\n",
    "        lr = 1e-4 # CNNs can handle a slightly higher learning rate\n",
    "        latent_dim = 20\n",
    "        cheby_order = 3\n",
    "        gamma = 0.05\n",
    "        delta = 0.5\n",
    "        grad_clip = 1.0 # CNNs are generally more stable\n",
    "        warmup_epochs = 5\n",
    "        run_name = 'ModelC_Conv_Optimized'\n",
    "\n",
    "    args = Args()\n",
    "    main(args)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Args:\n",
    "        # --- OPTIMIZED MODEL CONFIGURATION ---\n",
    "        model_type = 'D' # Run the new ConvChebyshevVAE\n",
    "        batch_size = 64\n",
    "        epochs = 20\n",
    "        lr = 1e-4 # CNNs can handle a slightly higher learning rate\n",
    "        latent_dim = 20\n",
    "        cheby_order = 3\n",
    "        gamma = 0.05\n",
    "        delta = 0.5\n",
    "        grad_clip = 1.0 # CNNs are generally more stable\n",
    "        warmup_epochs = 5\n",
    "        run_name = 'ModelD_Conv_Optimized'\n",
    "\n",
    "    args = Args()\n",
    "    main(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Args:\n",
    "        # --- OPTIMIZED MODEL CONFIGURATION ---\n",
    "        model_type = 'E' # Run the new ConvChebyshevVAE\n",
    "        batch_size = 64\n",
    "        epochs = 20\n",
    "        lr = 1e-4 # CNNs can handle a slightly higher learning rate\n",
    "        latent_dim = 20\n",
    "        cheby_order = 3\n",
    "        gamma = 0.05\n",
    "        delta = 0.5\n",
    "        grad_clip = 1.0 # CNNs are generally more stable\n",
    "        warmup_epochs = 5\n",
    "        run_name = 'ModelE_Conv_Optimized'\n",
    "\n",
    "    args = Args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d27d2-a68f-4f76-805d-b2db9b6ac5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================\n",
    "# Dataset\n",
    "# ============================\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # images in [0,1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# ============================\n",
    "# Loss function\n",
    "# ============================\n",
    "def vae_loss(recon_x, x, mu, log_var, beta=1.0):\n",
    "    # Handle both flat and image outputs\n",
    "    if recon_x.dim() == 2:  # Flat (baseline)\n",
    "        x = x.view(x.size(0), -1)\n",
    "    # recon_loss\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
    "    # KL\n",
    "    kl = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return recon_loss + beta * kl, recon_loss, kl\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Training function\n",
    "# ============================\n",
    "def train_vae(model, optimizer, scheduler, epochs=20, kl_anneal=True):  # More epochs\n",
    "    history = {\"loss\": [], \"recon\": [], \"kl\": []}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        recon_total, kl_total = 0, 0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, log_var = model(x)\n",
    "\n",
    "            # Cyclical beta annealing: 0 -> 1 -> 0.5 (prevents collapse)\n",
    "            if kl_anneal:\n",
    "                beta = 1.0 - 0.5 * abs((epoch - 10) / 10)  # Peaks at epoch 10\n",
    "                beta = max(0.1, min(1.0, beta))\n",
    "            else:\n",
    "                beta = 1.0\n",
    "            loss, recon_loss, kl = vae_loss(recon, x, mu, log_var, beta=beta)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Tighter clip\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # LR decay\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            recon_total += recon_loss.item()\n",
    "            kl_total += kl.item()\n",
    "\n",
    "        history[\"loss\"].append(train_loss / len(train_dataset))\n",
    "        history[\"recon\"].append(recon_total / len(train_dataset))\n",
    "        history[\"kl\"].append(kl_total / len(train_dataset))\n",
    "\n",
    "        print(f\"Epoch {epoch}: Loss={history['loss'][-1]:.4f}, \"\n",
    "              f\"Recon={history['recon'][-1]:.4f}, KL={history['kl'][-1]:.4f}, Beta={beta:.2f}, LR={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Train and compare models\n",
    "# ============================\n",
    "\n",
    "# BaselineVAE\n",
    "# Baseline (unchanged)\n",
    "baseline = BaselineVAE(latent_dim=32, input_dim=784)  # Ensure input_dim=784 for flat\n",
    "optimizer_base = torch.optim.AdamW(baseline.parameters(), lr=5e-4, weight_decay=1e-4)  # Tuned LR/weight_decay\n",
    "scheduler_base = CosineAnnealingLR(optimizer_base, T_max=20, eta_min=1e-6)\n",
    "print(\"\\nTraining BaselineVAE...\")\n",
    "history_base = train_vae(baseline, optimizer_base, scheduler_base, epochs=20)\n",
    "\n",
    "# Optimized ConvChebyshevVAE\n",
    "opt_conv_cheby = ConvChebyshevVAE(latent_dim=32, cheby_order=5)\n",
    "optimizer_cheby = torch.optim.AdamW(opt_conv_cheby.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler_cheby = CosineAnnealingLR(optimizer_cheby, T_max=20, eta_min=1e-6)\n",
    "print(\"\\nTraining Optimized ConvChebyshevVAE...\")\n",
    "history_cheby = train_vae(opt_conv_cheby, optimizer_cheby, scheduler_cheby, epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab41a3-634a-4521-9ff3-43ab32799f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "save_path= os.path.join(\"opti_results_new_1\", args.run_name)\n",
    "def plot_training_curves(history_base, history_cheby):\n",
    "    epochs = range(1, len(history_base[\"loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "\n",
    "    # Total loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, history_base[\"loss\"], label=\"BaselineVAE\")\n",
    "    plt.plot(epochs, history_cheby[\"loss\"], label=\"ConvChebyshevVAE\")\n",
    "    plt.title(\"Total Loss (ELBO)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Reconstruction loss\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, history_base[\"recon\"], label=\"BaselineVAE\")\n",
    "    plt.plot(epochs, history_cheby[\"recon\"], label=\"ConvChebyshevVAE\")\n",
    "    plt.title(\"Reconstruction Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"BCE Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # KL divergence\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, history_base[\"kl\"], label=\"BaselineVAE\")\n",
    "    plt.plot(epochs, history_cheby[\"kl\"], label=\"ConvChebyshevVAE\")\n",
    "    plt.title(\"KL Divergence\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"KL\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "# Call it after training\n",
    "plot_training_curves(history_base, history_cheby)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14183a7f-7031-4e81-bb14-628d83fbca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================\n",
    "# Assume all your model and loss function definitions from the notebook are here:\n",
    "# class PairedTransform: ...\n",
    "# class ChebyshevLayer(nn.Module): ...\n",
    "# class BaselineVAE(nn.Module): ...\n",
    "# class ChebyshevVAE(BaselineVAE): ...\n",
    "# class ConvChebyshevVAE(nn.Module): ...\n",
    "# def vae_loss_function(...): ...\n",
    "# def topological_loss(...): ...\n",
    "# def disentanglement_loss(...): ...\n",
    "# ============================\n",
    "\n",
    "# --- Device Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Data Loaders ---\n",
    "def get_dataloaders(batch_size, use_paired_transforms=False):\n",
    "    # Using FashionMNIST from your notebook for consistency\n",
    "    if use_paired_transforms:\n",
    "        # Paired transform for models using disentanglement loss\n",
    "        train_transform = PairedTransform(transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor()\n",
    "        ]))\n",
    "        train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=train_transform)\n",
    "    else:\n",
    "        # Standard transform\n",
    "        base_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=base_transform)\n",
    "\n",
    "    test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader, test_dataset\n",
    "\n",
    "# --- Modified VAE Loss from your `main` function logic ---\n",
    "def vae_loss(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD, BCE.item(), KLD.item()\n",
    "\n",
    "# --- Unified Training Function ---\n",
    "def train_model(model, model_name, use_paired, epochs=20):\n",
    "    print(f\"\\\\n=== Training Model {model_name} ===\")\n",
    "    \n",
    "    # Get the correct data loader\n",
    "    train_loader, test_loader, test_dataset = get_dataloaders(batch_size=64, use_paired_transforms=use_paired)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    history = {\"loss\": [], \"recon\": [], \"kl\": []}\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss, recon_total, kl_total = 0, 0, 0\n",
    "        \n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_paired:\n",
    "                (data1, data2), _ = data\n",
    "                x = data1.to(DEVICE)\n",
    "                # Note: Simplified for this example. The full disentanglement/topo loss logic from\n",
    "                # your `main` function would go here. We'll train with standard VAE loss for now.\n",
    "            else:\n",
    "                x, _ = data\n",
    "                x = x.to(DEVICE)\n",
    "\n",
    "            recon_x, mu, log_var = model(x)\n",
    "            loss, recon_l, kl_l = vae_loss(recon_x, x, mu, log_var)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            recon_total += recon_l\n",
    "            kl_total += kl_l\n",
    "            \n",
    "        avg_loss = train_loss / len(train_loader.dataset)\n",
    "        avg_recon = recon_total / len(train_loader.dataset)\n",
    "        avg_kl = kl_total / len(train_loader.dataset)\n",
    "\n",
    "        history[\"loss\"].append(avg_loss)\n",
    "        history[\"recon\"].append(avg_recon)\n",
    "        history[\"kl\"].append(avg_kl)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch}: Avg Loss={avg_loss:.4f}, Recon={avg_recon:.4f}, KL={avg_kl:.4f}\")\n",
    "            \n",
    "    return history, test_dataset\n",
    "\n",
    "\n",
    "# --- Visualization Function ---\n",
    "def show_reconstructions(model, model_name, dataset, n=8):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=n, shuffle=True)\n",
    "    x, _ = next(iter(loader))\n",
    "    x = x.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        recon, _, _ = model(x)\n",
    "    \n",
    "    print(f\"\\\\n{model_name} Reconstructions:\")\n",
    "    fig, axes = plt.subplots(2, n, figsize=(n * 2, 4))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        axes[0, i].imshow(x[i].cpu().squeeze(), cmap=\"gray\")\n",
    "        axes[0, i].set_title(\"Original\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        \n",
    "        # Reconstructed\n",
    "        axes[1, i].imshow(recon[i].cpu().view(28, 28), cmap=\"gray\")\n",
    "        axes[1, i].set_title(\"Recon\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "# ============================\n",
    "# Train and Evaluate All Models\n",
    "# ============================\n",
    "\n",
    "# --- Model A: BaselineVAE (Standard Loss) ---\n",
    "model_a = BaselineVAE(latent_dim=20, input_dim=784)\n",
    "history_a, test_dataset_a = train_model(model_a, \"A: BaselineVAE\", use_paired=False)\n",
    "show_reconstructions(model_a, \"Model A\", test_dataset_a)\n",
    "\n",
    "# --- Model B: BaselineVAE (Advanced Loss Training) ---\n",
    "model_b = BaselineVAE(latent_dim=20, input_dim=784)\n",
    "history_b, test_dataset_b = train_model(model_b, \"B: BaselineVAE + Advanced Loss\", use_paired=True)\n",
    "show_reconstructions(model_b, \"Model B\", test_dataset_b)\n",
    "\n",
    "# --- Model C: ChebyshevVAE (Standard Loss) ---\n",
    "model_c = ChebyshevVAE(latent_dim=20, input_dim=784, cheby_order=3)\n",
    "history_c, test_dataset_c = train_model(model_c, \"C: ChebyshevVAE\", use_paired=False)\n",
    "show_reconstructions(model_c, \"Model C\", test_dataset_c)\n",
    "\n",
    "# --- Model D: ChebyshevVAE (Advanced Loss Training) ---\n",
    "model_d = ChebyshevVAE(latent_dim=20, input_dim=784, cheby_order=3)\n",
    "history_d, test_dataset_d = train_model(model_d, \"D: ChebyshevVAE + Advanced Loss\", use_paired=True)\n",
    "show_reconstructions(model_d, \"Model D\", test_dataset_d)\n",
    "\n",
    "# --- Model E: ConvChebyshevVAE (Advanced Loss Training) ---\n",
    "# Note: ConvVAE needs a different vae_loss logic for shape, handled by a more robust function\n",
    "model_e = ConvChebyshevVAE(latent_dim=20, cheby_order=3)\n",
    "history_e, test_dataset_e = train_model(model_e, \"E: ConvChebyshevVAE + Advanced Loss\", use_paired=True)\n",
    "show_reconstructions(model_e, \"Model E\", test_dataset_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd346d1-61e0-4809-b13e-ce5d07c8e6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5dc7c8-9ecc-4fb0-9200-340266e6eaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093c6b3-84ad-455e-84f6-d357160afc93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c7ace16-b01c-44f6-a2d8-81f194af5885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f710756-cb8c-41fa-87d9-48f7861eb976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173a895-10cb-4957-bf99-370bad571a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599788e6-76f0-476b-b9bf-462065159331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc187e1-806b-4c63-9887-df6f4015d503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nn_env)",
   "language": "python",
   "name": "nn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
