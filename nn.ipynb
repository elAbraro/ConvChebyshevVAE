{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038cfa7a-d694-47c2-94ec-653744dcbcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c4aae5a9-53b6-4e90-b35c-e2bf45b3e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n",
      "PyTorch CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "# If available, this will show the version PyTorch was built with\n",
    "print(f\"PyTorch CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d3b963b6-e5c4-4fbb-80eb-dd8969b24653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.utils.deprecation\")\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.diagrams import PairwiseDistance\n",
    "\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "eae5eb28-dbcc-49c1-ada3-cdec6965aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChebyshevLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of the Adaptive Neuron using Chebyshev Polynomials.\n",
    "    The neuron's weight is a function of the input, modeled by a polynomial expansion.\n",
    "    The output is y = sum(x_i * w_i(x_i)), where w_i(x_i) is the adaptive weight.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, order=3):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.order = order\n",
    "        \n",
    "        # Learnable coefficients for the Chebyshev polynomials\n",
    "        # Shape: (out_features, in_features, order + 1)\n",
    "        self.coeffs = nn.Parameter(torch.empty(out_features, in_features, order + 1))\n",
    "        nn.init.xavier_uniform_(self.coeffs)\n",
    "\n",
    "    # In your project.py file, inside the ChebyshevLayer class\n",
    "\n",
    "    def forward(self, x):\n",
    "        # IMPORTANT: Input x must be scaled to the [-1, 1] range.\n",
    "        \n",
    "        # --- FIX STARTS HERE ---\n",
    "        # Avoid in-place modification by building a list of polynomial tensors\n",
    "        cheby_poly_list = []\n",
    "        \n",
    "        # T_0(x) = 1\n",
    "        t0 = torch.ones_like(x)\n",
    "        cheby_poly_list.append(t0)\n",
    "        \n",
    "        if self.order > 0:\n",
    "            # T_1(x) = x\n",
    "            t1 = x\n",
    "            cheby_poly_list.append(t1)\n",
    "        \n",
    "        # Recursively compute higher-order polynomials\n",
    "        for k in range(2, self.order + 1):\n",
    "            tk = 2 * x * cheby_poly_list[k - 1] - cheby_poly_list[k - 2]\n",
    "            cheby_poly_list.append(tk)\n",
    "\n",
    "        # Stack the list of tensors into a single tensor along a new dimension\n",
    "        # Shape becomes: (batch_size, in_features, order + 1)\n",
    "        cheby_polys = torch.stack(cheby_poly_list, dim=2)\n",
    "        # --- FIX ENDS HERE ---\n",
    "\n",
    "        # The rest of the function remains the same\n",
    "        adaptive_weights = torch.einsum('oik,bik->boi', self.coeffs, cheby_polys)\n",
    "        output = torch.einsum('bi,boi->bo', x, adaptive_weights)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6a579c27-7646-470a-9744-644470101a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineVAE(nn.Module):\n",
    "    \"\"\"A standard VAE with a simple MLP encoder and decoder.\"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid() # To output pixel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_log_var(h)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b36b5ca4-b618-4478-85ba-73cebf70f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChebyshevVAE(BaselineVAE):\n",
    "    \"\"\"\n",
    "    A VAE using ChebyshevLayers.\n",
    "    CORRECTED VERSION: Uses tanh activation to ensure inputs to subsequent\n",
    "    ChebyshevLayers are in the stable [-1, 1] range.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20, cheby_order=3):\n",
    "        super().__init__(input_dim, hidden_dim, latent_dim)\n",
    "        # Override encoder and decoder with ChebyshevLayers\n",
    "        self.encoder_cheby = ChebyshevLayer(input_dim, hidden_dim, order=cheby_order)\n",
    "        self.fc_mu = ChebyshevLayer(hidden_dim, latent_dim, order=cheby_order)\n",
    "        self.fc_log_var = ChebyshevLayer(hidden_dim, latent_dim, order=cheby_order)\n",
    "        \n",
    "        self.decoder_cheby1 = ChebyshevLayer(latent_dim, hidden_dim, order=cheby_order)\n",
    "        # The final layer remains linear with sigmoid for reconstruction.\n",
    "        self.decoder_final = nn.Sequential(nn.Linear(hidden_dim, input_dim), nn.Sigmoid())\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Apply tanh to constrain the output of the first layer to [-1, 1]\n",
    "        # before passing it to the next Chebyshev layers.\n",
    "        h = torch.tanh(self.encoder_cheby(x))\n",
    "        return self.fc_mu(h), self.fc_log_var(h)\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Also apply tanh here for consistency\n",
    "        h = torch.tanh(self.decoder_cheby1(z))\n",
    "        return self.decoder_final(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d69a4e6a-85e6-4cbc-8ff3-1134b4c95329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(recon_x, x, mu, log_var):\n",
    "    \"\"\"Standard VAE loss with shape and device checks.\"\"\"\n",
    "    # Ensure same device\n",
    "    assert recon_x.device == x.device, f\"Device mismatch: recon_x on {recon_x.device}, x on {x.device}\"\n",
    "    \n",
    "    # Reverse normalization for x if needed (for paired transforms)\n",
    "    x = x * 0.5 + 0.5  # Transform from [-1, 1] to [0, 1]\n",
    "    x = x.view(-1, 784)\n",
    "    recon_x = recon_x.view(-1, 784)\n",
    "    \n",
    "    # Check shapes\n",
    "    assert recon_x.shape == x.shape, f\"Shape mismatch: recon_x {recon_x.shape}, x {x.shape}\"\n",
    "    \n",
    "    # Check value ranges\n",
    "    assert (recon_x >= 0).all() and (recon_x <= 1).all(), \"recon_x values out of [0, 1] range\"\n",
    "    assert (x >= 0).all() and (x <= 1).all(), \"x values out of [0, 1] range\"\n",
    "    \n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "267e1702-685f-42ad-9b42-fbbc0719a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import persim\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def topological_loss(X_batch, Z_batch):\n",
    "    \"\"\"\n",
    "    Calculates the topological loss using persistence diagrams.\n",
    "    NEW APPROACH: Uses the `persim` library for a more robust distance calculation\n",
    "    that correctly handles diagrams with different numbers of points.\n",
    "    \"\"\"\n",
    "    # 1. Prepare data and reshape for giotto-tda\n",
    "    X_np = X_batch.view(X_batch.shape[0], -1).detach().cpu().numpy()\n",
    "    Z_np = Z_batch.detach().cpu().numpy()\n",
    "    \n",
    "    X_point_cloud = X_np[None, :, :]\n",
    "    Z_point_cloud = Z_np[None, :, :]\n",
    "    \n",
    "    # 2. Compute persistence diagrams using giotto-tda (this part works fine)\n",
    "    vrp = VietorisRipsPersistence(homology_dimensions=[0, 1])\n",
    "    X_diag = vrp.fit_transform(X_point_cloud)[0]\n",
    "    Z_diag = vrp.fit_transform(Z_point_cloud)[0]\n",
    "    \n",
    "    # 3. Compute distance using persim, separately for each homology dimension\n",
    "    total_distance = 0.0\n",
    "    homology_dims = [0, 1] # H0 (components) and H1 (holes)\n",
    "\n",
    "    for dim in homology_dims:\n",
    "        # Filter the diagrams to get points for the current dimension\n",
    "        X_diag_dim = X_diag[X_diag[:, 2] == dim]\n",
    "        Z_diag_dim = Z_diag[Z_diag[:, 2] == dim]\n",
    "        \n",
    "        # persim expects (n_points, 2) arrays (birth, death), so we slice\n",
    "        X_birth_death = X_diag_dim[:, :2]\n",
    "        Z_birth_death = Z_diag_dim[:, :2]\n",
    "        \n",
    "        # Handle the case where one diagram has no features for a dimension\n",
    "        if X_birth_death.shape[0] == 0 and Z_birth_death.shape[0] == 0:\n",
    "            distance_dim = 0.0\n",
    "        else:\n",
    "            distance_dim = persim.wasserstein(X_birth_death, Z_birth_death)\n",
    "        \n",
    "        total_distance += distance_dim\n",
    "        \n",
    "    return torch.tensor(total_distance, device=X_batch.device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7fe9e012-38f9-4338-bc8b-70ddacbd042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disentanglement_loss(z_shared1, z_distinct1, z_shared2, z_distinct2):\n",
    "    \"\"\"\n",
    "    Self-supervised loss for disentanglement.\n",
    "    Your Idea 3: Self-Supervised Learning for Disentangled Dimensionality Reduction.\n",
    "    \"\"\"\n",
    "    # 1. Force shared representations to be similar (cosine similarity)\n",
    "    loss_shared = 1 - nn.functional.cosine_similarity(z_shared1, z_shared2, dim=-1).mean()\n",
    "    \n",
    "    # 2. Force distinct representations to be dissimilar (simple contrastive loss)\n",
    "    # This is a simplified version. A more robust implementation would use InfoNCE loss.\n",
    "    pdist = nn.PairwiseDistance(p=2)\n",
    "    loss_distinct = -pdist(z_distinct1, z_distinct2).mean() # Push them apart\n",
    "    \n",
    "    return loss_shared + loss_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "193cadef-9cc2-4a9e-b495-72fe837544d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedTransform:\n",
    "    \"\"\"A transform that returns two different augmented views of the same image.\"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.transform(x), self.transform(x)\n",
    "\n",
    "def get_dataloaders(batch_size, use_paired_transforms=False):\n",
    "    # Only use ToTensor() to scale data to the [0, 1] range.\n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    if use_paired_transforms:\n",
    "        # Augmentations for disentanglement loss\n",
    "        paired_transform = PairedTransform(\n",
    "            transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,)),\n",
    "            ])\n",
    "        )\n",
    "        train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=paired_transform)\n",
    "    else:\n",
    "        train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=base_transform)\n",
    "\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=base_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "raw",
   "id": "909249a6-0208-4037-8c80-a09635c80a9d",
   "metadata": {},
   "source": [
    "print('sa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3b553d93-9503-4759-bdcb-85520a837065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================\n",
    "# == 4. VISUALIZATION UTILITIES\n",
    "# ==========================================================================================\n",
    "def save_reconstructions(model, test_loader, device, save_path):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data, _ = next(iter(test_loader))\n",
    "        data = data.to(device)\n",
    "        recon, _, _ = model(data)\n",
    "        \n",
    "        comparison = torch.cat([data.view(-1, 1, 28, 28)[:8], \n",
    "                                recon.view(-1, 1, 28, 28)[:8]])\n",
    "        \n",
    "        # De-normalize from [-1, 1] to [0, 1]\n",
    "        comparison = comparison * 0.5 + 0.5\n",
    "        \n",
    "        from torchvision.utils import save_image\n",
    "        save_image(comparison.cpu(), save_path, nrow=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9a051811-05b8-40a2-9f3d-ad8278b81b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_samples(model, device, save_path, num_samples=64):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, model.latent_dim).to(device)\n",
    "        samples = model.decode(z).cpu()\n",
    "\n",
    "        # De-normalize\n",
    "        samples = samples * 0.5 + 0.5\n",
    "        \n",
    "        from torchvision.utils import save_image\n",
    "        save_image(samples.view(num_samples, 1, 28, 28), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "65deb405-5d1a-4d82-a5e1-a0a9d1873781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_latent_space(model, test_loader, device, save_path):\n",
    "    model.eval()\n",
    "    all_z = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.to(device)\n",
    "            mu, _ = model.encode(data.view(-1, 784))\n",
    "            all_z.append(mu.cpu().numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "\n",
    "    all_z = np.concatenate(all_z, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Use UMAP for dimensionality reduction\n",
    "    reducer = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "    embedding = reducer.fit_transform(all_z)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], c=all_labels, cmap='Spectral', s=5)\n",
    "    plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "    plt.title('UMAP Projection of the Latent Space')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "06f9d401-1d6d-4995-9cfc-df5c9bb751cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create results directory\n",
    "    results_dir = os.path.join(\"results\", args.run_name)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    use_paired = args.model_type in ['B', 'D']\n",
    "    train_loader, test_loader = get_dataloaders(args.batch_size, use_paired_transforms=use_paired)\n",
    "\n",
    "    # --- Model Selection ---\n",
    "    if args.model_type in ['A', 'B']:\n",
    "        model = BaselineVAE(latent_dim=args.latent_dim).to(device)\n",
    "        print(\"Initialized Baseline VAE (Model A/B)\")\n",
    "    elif args.model_type in ['C', 'D']:\n",
    "        model = ChebyshevVAE(latent_dim=args.latent_dim, cheby_order=args.cheby_order).to(device)\n",
    "        print(f\"Initialized Chebyshev-VAE (Model C/D) with order {args.cheby_order}\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type specified.\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    # --- Training Loop ---\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_paired:\n",
    "                # Paired data for disentanglement\n",
    "                (data1, data2), _ = data\n",
    "                data1, data2 = data1.to(device), data2.to(device)\n",
    "                \n",
    "                # Verify input shapes\n",
    "                assert data1.shape == data2.shape, f\"Shape mismatch: data1 {data1.shape}, data2 {data2.shape}\"\n",
    "                \n",
    "                # Forward pass on first view\n",
    "                recon, mu, log_var = model(data1)\n",
    "                \n",
    "                # --- Calculate Hybrid Loss (Model B or D) ---\n",
    "                loss_v = vae_loss_function(recon, data1, mu, log_var)\n",
    "                \n",
    "                # Calculate topological loss only periodically to save time\n",
    "                if batch_idx % 20 == 0: # Calculate every 20 batches\n",
    "                    z = model.reparameterize(mu, log_var)\n",
    "                    loss_t = topological_loss(data1, z)\n",
    "                else:\n",
    "                    # On other batches, set it to zero so it doesn't affect the gradient\n",
    "                    loss_t = torch.tensor(0.0, device=data1.device) \n",
    "                \n",
    "                # Disentanglement Loss (this one is fast, can run every time)\n",
    "                mu1, _ = model.encode(data1.view(-1, 784))\n",
    "                mu2, _ = model.encode(data2.view(-1, 784))\n",
    "                z_shared1, z_distinct1 = torch.chunk(mu1, 2, dim=1)\n",
    "                z_shared2, z_distinct2 = torch.chunk(mu2, 2, dim=1)\n",
    "                loss_d = disentanglement_loss(z_shared1, z_distinct1, z_shared2, z_distinct2)\n",
    "                \n",
    "                loss = loss_v + args.gamma * loss_t + args.delta * loss_d\n",
    "            else:\n",
    "                # Standard training (Model A or C)\n",
    "                data, _ = data\n",
    "                data = data.to(device)\n",
    "                recon, mu, log_var = model(data)\n",
    "                loss = vae_loss_function(recon, data, mu, log_var)\n",
    "\n",
    "            loss.backward()\n",
    "            total_train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, _ in test_loader:\n",
    "                data = data.to(device)\n",
    "                recon, mu, log_var = model(data)\n",
    "                total_test_loss += vae_loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "        avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(f'====> Epoch: {epoch} Average train loss: {avg_train_loss:.4f} | Average test loss: {avg_test_loss:.4f}')\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # --- Save Artifacts ---\n",
    "    print(\"Saving model and generating visualizations...\")\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(results_dir, \"model.pth\"))\n",
    "\n",
    "    # Save loss curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_dir, 'loss_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Save visualizations\n",
    "    save_reconstructions(model, test_loader, device, os.path.join(results_dir, 'reconstructions.png'))\n",
    "    save_generated_samples(model, device, os.path.join(results_dir, 'generated_samples.png'))\n",
    "    save_latent_space(model, test_loader, device, os.path.join(results_dir, 'latent_space.png'))\n",
    "\n",
    "    print(f\"Results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c28fb2d5-ecce-4398-90ad-642a6b7b6f53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initialized Chebyshev-VAE (Model C/D) with order 3\n",
      "====> Epoch: 1 Average train loss: 320.1696 | Average test loss: 835.1429\n",
      "====> Epoch: 2 Average train loss: 373.1273 | Average test loss: 863.2622\n",
      "====> Epoch: 3 Average train loss: 390.0425 | Average test loss: 807.6828\n",
      "====> Epoch: 4 Average train loss: 399.4748 | Average test loss: 1142.4739\n",
      "====> Epoch: 5 Average train loss: 448.0072 | Average test loss: 826.0711\n",
      "====> Epoch: 6 Average train loss: 469.6598 | Average test loss: 851.1818\n",
      "====> Epoch: 7 Average train loss: 441.8610 | Average test loss: 1119.5208\n",
      "====> Epoch: 8 Average train loss: 413.6027 | Average test loss: 876.8319\n",
      "====> Epoch: 9 Average train loss: 474.1185 | Average test loss: 1117.7382\n",
      "====> Epoch: 10 Average train loss: 423.2540 | Average test loss: 978.8857\n",
      "====> Epoch: 11 Average train loss: 473.8053 | Average test loss: 1048.2882\n",
      "====> Epoch: 12 Average train loss: 414.3477 | Average test loss: 1034.1915\n",
      "====> Epoch: 13 Average train loss: 509.6666 | Average test loss: 976.0951\n",
      "====> Epoch: 14 Average train loss: 537.6600 | Average test loss: 1089.4959\n",
      "====> Epoch: 15 Average train loss: 476.8221 | Average test loss: 1014.1912\n",
      "====> Epoch: 16 Average train loss: 569.1044 | Average test loss: 1040.0543\n",
      "====> Epoch: 17 Average train loss: 455.3442 | Average test loss: 1135.3259\n",
      "====> Epoch: 18 Average train loss: 478.3879 | Average test loss: 959.7145\n",
      "====> Epoch: 19 Average train loss: 476.9768 | Average test loss: 977.8284\n",
      "====> Epoch: 20 Average train loss: 498.3553 | Average test loss: 1449.6385\n",
      "Training finished.\n",
      "Saving model and generating visualizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T2510570\\anaconda3\\envs\\nn_env\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results\\ModelD_Final_Run\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Args:\n",
    "        model_type = 'D'\n",
    "        batch_size = 32  # Reduced for performance\n",
    "        epochs = 20\n",
    "        lr = 1e-3\n",
    "        latent_dim = 20\n",
    "        cheby_order = 3\n",
    "        gamma = 0.1\n",
    "        delta = 1.0\n",
    "        run_name = 'ModelD_Final_Run'\n",
    "    \n",
    "    args = Args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6887afe-932b-47be-928f-8db3198405e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3d174-71a5-46fe-b818-579f6ab4fe59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nn_env)",
   "language": "python",
   "name": "nn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
