{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038cfa7a-d694-47c2-94ec-653744dcbcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4aae5a9-53b6-4e90-b35c-e2bf45b3e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n",
      "PyTorch CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "# If available, this will show the version PyTorch was built with\n",
    "print(f\"PyTorch CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3b963b6-e5c4-4fbb-80eb-dd8969b24653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.utils.deprecation\")\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.diagrams import PairwiseDistance\n",
    "\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eae5eb28-dbcc-49c1-ada3-cdec6965aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChebyshevLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of the Adaptive Neuron using Chebyshev Polynomials.\n",
    "    The neuron's weight is a function of the input, modeled by a polynomial expansion.\n",
    "    The output is y = sum(x_i * w_i(x_i)), where w_i(x_i) is the adaptive weight.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, order=3):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.order = order\n",
    "        \n",
    "        # Learnable coefficients for the Chebyshev polynomials\n",
    "        # Shape: (out_features, in_features, order + 1)\n",
    "        self.coeffs = nn.Parameter(torch.empty(out_features, in_features, order + 1))\n",
    "        nn.init.xavier_uniform_(self.coeffs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape of x: (batch_size, in_features)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Calculate Chebyshev polynomials T_0, T_1, ..., T_k for each input feature\n",
    "        # Shape: (batch_size, in_features, order + 1)\n",
    "        cheby_polys = torch.empty(batch_size, self.in_features, self.order + 1, device=x.device)\n",
    "        cheby_polys[..., 0] = torch.ones_like(x)  # T_0(x) = 1\n",
    "        if self.order > 0:\n",
    "            cheby_polys[..., 1] = x  # T_1(x) = x\n",
    "        for k in range(2, self.order + 1):\n",
    "            # Recursive definition: T_k(x) = 2x * T_{k-1}(x) - T_{k-2}(x)\n",
    "            cheby_polys[..., k] = 2 * x * cheby_polys[..., k - 1] - cheby_polys[..., k - 2]\n",
    "        \n",
    "        # Compute adaptive weights: w_i(x_i) = Σ c_ij * T_j(x_i)\n",
    "        # c: (out, in, order) * T: (batch, in, order) -> w: (batch, out, in)\n",
    "        adaptive_weights = torch.einsum('oik,bik->boi', self.coeffs, cheby_polys)\n",
    "        \n",
    "        # Compute the final output: y_j = Σ_i x_i * w_ji(x_i)\n",
    "        # x_expanded: (batch, 1, in) * w: (batch, out, in) -> y: (batch, out)\n",
    "        output = torch.einsum('bi,boi->bo', x, adaptive_weights)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a579c27-7646-470a-9744-644470101a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineVAE(nn.Module):\n",
    "    \"\"\"A standard VAE with a simple MLP encoder and decoder.\"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid() # To output pixel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_log_var(h)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b36b5ca4-b618-4478-85ba-73cebf70f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChebyshevVAE(BaselineVAE):\n",
    "    \"\"\"\n",
    "    A VAE using ChebyshevLayers.\n",
    "    CORRECTED VERSION: Uses tanh activation to ensure inputs to subsequent\n",
    "    ChebyshevLayers are in the stable [-1, 1] range.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20, cheby_order=3):\n",
    "        super().__init__(input_dim, hidden_dim, latent_dim)\n",
    "        # Override encoder and decoder with ChebyshevLayers\n",
    "        self.encoder_cheby = ChebyshevLayer(input_dim, hidden_dim, order=cheby_order)\n",
    "        self.fc_mu = ChebyshevLayer(hidden_dim, latent_dim, order=cheby_order)\n",
    "        self.fc_log_var = ChebyshevLayer(hidden_dim, latent_dim, order=cheby_order)\n",
    "        \n",
    "        self.decoder_cheby1 = ChebyshevLayer(latent_dim, hidden_dim, order=cheby_order)\n",
    "        # The final layer remains linear with sigmoid for reconstruction.\n",
    "        self.decoder_final = nn.Sequential(nn.Linear(hidden_dim, input_dim), nn.Sigmoid())\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Apply tanh to constrain the output of the first layer to [-1, 1]\n",
    "        # before passing it to the next Chebyshev layers.\n",
    "        h = torch.tanh(self.encoder_cheby(x))\n",
    "        return self.fc_mu(h), self.fc_log_var(h)\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Also apply tanh here for consistency\n",
    "        h = torch.tanh(self.decoder_cheby1(z))\n",
    "        return self.decoder_final(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d69a4e6a-85e6-4cbc-8ff3-1134b4c95329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(recon_x, x, mu, log_var):\n",
    "    \"\"\"Standard VAE loss with shape and device checks.\"\"\"\n",
    "    # Ensure same device\n",
    "    assert recon_x.device == x.device, f\"Device mismatch: recon_x on {recon_x.device}, x on {x.device}\"\n",
    "    \n",
    "    # Reverse normalization for x if needed (for paired transforms)\n",
    "    x = x * 0.5 + 0.5  # Transform from [-1, 1] to [0, 1]\n",
    "    x = x.view(-1, 784)\n",
    "    recon_x = recon_x.view(-1, 784)\n",
    "    \n",
    "    # Check shapes\n",
    "    assert recon_x.shape == x.shape, f\"Shape mismatch: recon_x {recon_x.shape}, x {x.shape}\"\n",
    "    \n",
    "    # Check value ranges\n",
    "    assert (recon_x >= 0).all() and (recon_x <= 1).all(), \"recon_x values out of [0, 1] range\"\n",
    "    assert (x >= 0).all() and (x <= 1).all(), \"x values out of [0, 1] range\"\n",
    "    \n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "267e1702-685f-42ad-9b42-fbbc0719a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.diagrams import PairwiseDistance\n",
    "\n",
    "vr = VietorisRipsPersistence(homology_dimensions=[0, 1])\n",
    "wasserstein = PairwiseDistance(metric=\"wasserstein\")\n",
    "\n",
    "def topological_loss(X_batch, Z_batch):\n",
    "    # Flatten into point clouds (batch, n_points, n_features)\n",
    "    X_np = X_batch.detach().cpu().numpy().reshape(X_batch.shape[0], -1, 1)\n",
    "    Z_np = Z_batch.detach().cpu().numpy().reshape(Z_batch.shape[0], -1, 1)\n",
    "\n",
    "    # Compute persistence diagrams\n",
    "    X_diag = vr.fit_transform(X_np)\n",
    "    Z_diag = vr.fit_transform(Z_np)\n",
    "\n",
    "    # Take the first sample from each batch and stack them\n",
    "    diagrams = np.stack([X_diag[0], Z_diag[0]], axis=0)  # shape (2, n_points, 2)\n",
    "\n",
    "    # Compute Wasserstein distance\n",
    "    distance_matrix = wasserstein.fit_transform(diagrams)\n",
    "    distance = distance_matrix[0, 1]\n",
    "\n",
    "    return torch.tensor(distance, device=X_batch.device, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7fe9e012-38f9-4338-bc8b-70ddacbd042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disentanglement_loss(z_shared1, z_distinct1, z_shared2, z_distinct2):\n",
    "    \"\"\"\n",
    "    Self-supervised loss for disentanglement.\n",
    "    Your Idea 3: Self-Supervised Learning for Disentangled Dimensionality Reduction.\n",
    "    \"\"\"\n",
    "    # 1. Force shared representations to be similar (cosine similarity)\n",
    "    loss_shared = 1 - nn.functional.cosine_similarity(z_shared1, z_shared2, dim=-1).mean()\n",
    "    \n",
    "    # 2. Force distinct representations to be dissimilar (simple contrastive loss)\n",
    "    # This is a simplified version. A more robust implementation would use InfoNCE loss.\n",
    "    pdist = nn.PairwiseDistance(p=2)\n",
    "    loss_distinct = -pdist(z_distinct1, z_distinct2).mean() # Push them apart\n",
    "    \n",
    "    return loss_shared + loss_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "193cadef-9cc2-4a9e-b495-72fe837544d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedTransform:\n",
    "    \"\"\"A transform that returns two different augmented views of the same image.\"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.transform(x), self.transform(x)\n",
    "\n",
    "def get_dataloaders(batch_size, use_paired_transforms=False):\n",
    "    # Only use ToTensor() to scale data to the [0, 1] range.\n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    if use_paired_transforms:\n",
    "        # Augmentations for disentanglement loss\n",
    "        paired_transform = PairedTransform(\n",
    "            transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,)),\n",
    "            ])\n",
    "        )\n",
    "        train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=paired_transform)\n",
    "    else:\n",
    "        train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=base_transform)\n",
    "\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=base_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "raw",
   "id": "909249a6-0208-4037-8c80-a09635c80a9d",
   "metadata": {},
   "source": [
    "print('sa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b553d93-9503-4759-bdcb-85520a837065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================\n",
    "# == 4. VISUALIZATION UTILITIES\n",
    "# ==========================================================================================\n",
    "def save_reconstructions(model, test_loader, device, save_path):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data, _ = next(iter(test_loader))\n",
    "        data = data.to(device)\n",
    "        recon, _, _ = model(data)\n",
    "        \n",
    "        comparison = torch.cat([data.view(-1, 1, 28, 28)[:8], \n",
    "                                recon.view(-1, 1, 28, 28)[:8]])\n",
    "        \n",
    "        # De-normalize from [-1, 1] to [0, 1]\n",
    "        comparison = comparison * 0.5 + 0.5\n",
    "        \n",
    "        from torchvision.utils import save_image\n",
    "        save_image(comparison.cpu(), save_path, nrow=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a051811-05b8-40a2-9f3d-ad8278b81b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_samples(model, device, save_path, num_samples=64):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, model.latent_dim).to(device)\n",
    "        samples = model.decode(z).cpu()\n",
    "\n",
    "        # De-normalize\n",
    "        samples = samples * 0.5 + 0.5\n",
    "        \n",
    "        from torchvision.utils import save_image\n",
    "        save_image(samples.view(num_samples, 1, 28, 28), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "65deb405-5d1a-4d82-a5e1-a0a9d1873781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_latent_space(model, test_loader, device, save_path):\n",
    "    model.eval()\n",
    "    all_z = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.to(device)\n",
    "            mu, _ = model.encode(data.view(-1, 784))\n",
    "            all_z.append(mu.cpu().numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "\n",
    "    all_z = np.concatenate(all_z, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Use UMAP for dimensionality reduction\n",
    "    reducer = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "    embedding = reducer.fit_transform(all_z)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], c=all_labels, cmap='Spectral', s=5)\n",
    "    plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "    plt.title('UMAP Projection of the Latent Space')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "06f9d401-1d6d-4995-9cfc-df5c9bb751cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create results directory\n",
    "    results_dir = os.path.join(\"results\", args.run_name)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    use_paired = args.model_type in ['B', 'D']\n",
    "    train_loader, test_loader = get_dataloaders(args.batch_size, use_paired_transforms=use_paired)\n",
    "\n",
    "    # --- Model Selection ---\n",
    "    if args.model_type in ['A', 'B']:\n",
    "        model = BaselineVAE(latent_dim=args.latent_dim).to(device)\n",
    "        print(\"Initialized Baseline VAE (Model A/B)\")\n",
    "    elif args.model_type in ['C', 'D']:\n",
    "        model = ChebyshevVAE(latent_dim=args.latent_dim, cheby_order=args.cheby_order).to(device)\n",
    "        print(f\"Initialized Chebyshev-VAE (Model C/D) with order {args.cheby_order}\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type specified.\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    # --- Training Loop ---\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_paired:\n",
    "                # Paired data for disentanglement\n",
    "                (data1, data2), _ = data\n",
    "                data1, data2 = data1.to(device), data2.to(device)\n",
    "                \n",
    "                # Verify input shapes\n",
    "                assert data1.shape == data2.shape, f\"Shape mismatch: data1 {data1.shape}, data2 {data2.shape}\"\n",
    "                \n",
    "                # Forward pass on first view\n",
    "                recon, mu, log_var = model(data1)\n",
    "                \n",
    "                # --- Calculate Hybrid Loss (Model B or D) ---\n",
    "                loss_v = vae_loss_function(recon, data1, mu, log_var)\n",
    "                \n",
    "                # Calculate topological loss only periodically to save time\n",
    "                if batch_idx % 20 == 0: # Calculate every 20 batches\n",
    "                    z = model.reparameterize(mu, log_var)\n",
    "                    loss_t = topological_loss(data1, z)\n",
    "                else:\n",
    "                    # On other batches, set it to zero so it doesn't affect the gradient\n",
    "                    loss_t = torch.tensor(0.0, device=data1.device) \n",
    "                \n",
    "                # Disentanglement Loss (this one is fast, can run every time)\n",
    "                mu1, _ = model.encode(data1.view(-1, 784))\n",
    "                mu2, _ = model.encode(data2.view(-1, 784))\n",
    "                z_shared1, z_distinct1 = torch.chunk(mu1, 2, dim=1)\n",
    "                z_shared2, z_distinct2 = torch.chunk(mu2, 2, dim=1)\n",
    "                loss_d = disentanglement_loss(z_shared1, z_distinct1, z_shared2, z_distinct2)\n",
    "                \n",
    "                loss = loss_v + args.gamma * loss_t + args.delta * loss_d\n",
    "            else:\n",
    "                # Standard training (Model A or C)\n",
    "                data, _ = data\n",
    "                data = data.to(device)\n",
    "                recon, mu, log_var = model(data)\n",
    "                loss = vae_loss_function(recon, data, mu, log_var)\n",
    "\n",
    "            loss.backward()\n",
    "            total_train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, _ in test_loader:\n",
    "                data = data.to(device)\n",
    "                recon, mu, log_var = model(data)\n",
    "                total_test_loss += vae_loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "        avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(f'====> Epoch: {epoch} Average train loss: {avg_train_loss:.4f} | Average test loss: {avg_test_loss:.4f}')\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # --- Save Artifacts ---\n",
    "    print(\"Saving model and generating visualizations...\")\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(results_dir, \"model.pth\"))\n",
    "\n",
    "    # Save loss curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_dir, 'loss_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Save visualizations\n",
    "    save_reconstructions(model, test_loader, device, os.path.join(results_dir, 'reconstructions.png'))\n",
    "    save_generated_samples(model, device, os.path.join(results_dir, 'generated_samples.png'))\n",
    "    save_latent_space(model, test_loader, device, os.path.join(results_dir, 'latent_space.png'))\n",
    "\n",
    "    print(f\"Results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c28fb2d5-ecce-4398-90ad-642a6b7b6f53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initialized Chebyshev-VAE (Model C/D) with order 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModelD_Final_Run\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[1;32m---> 17\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 50\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# Calculate every 20 batches\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mreparameterize(mu, log_var)\n\u001b[1;32m---> 50\u001b[0m     loss_t \u001b[38;5;241m=\u001b[39m \u001b[43mtopological_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# On other batches, set it to zero so it doesn't affect the gradient\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     loss_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdata1\u001b[38;5;241m.\u001b[39mdevice) \n",
      "Cell \u001b[1;32mIn[64], line 21\u001b[0m, in \u001b[0;36mtopological_loss\u001b[1;34m(X_batch, Z_batch)\u001b[0m\n\u001b[0;32m     18\u001b[0m Z_diag \u001b[38;5;241m=\u001b[39m vr\u001b[38;5;241m.\u001b[39mfit_transform(Z_np)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Compute Wasserstein distance between the first diagram of each\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m distance_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mwasserstein\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_diag\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ_diag\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m distance \u001b[38;5;241m=\u001b[39m distance_matrix[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(distance, device\u001b[38;5;241m=\u001b[39mX_batch\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn_env\\lib\\site-packages\\gtda\\utils\\_docs.py:106\u001b[0m, in \u001b[0;36madapt_fit_transform_docs.<locals>.make_new_fit_transform.<locals>.fit_transform_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(original_fit_transform)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fit_transform(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn_env\\lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn_env\\lib\\site-packages\\gtda\\diagrams\\distance.py:171\u001b[0m, in \u001b[0;36mPairwiseDistance.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Store all observed homology dimensions in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    :attr:`homology_dimensions_` and compute\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m    :attr:`effective_metric_params_`. Then, return the estimator.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_diagrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     validate_params(\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hyperparameters, exclude\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn_env\\lib\\site-packages\\gtda\\utils\\validation.py:50\u001b[0m, in \u001b[0;36mcheck_diagrams\u001b[1;34m(X, copy)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_diagrams\u001b[39m(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Input validation for collections of persistence diagrams.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    Basic type and sanity checks are run on the input collection and the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     X_array \u001b[38;5;241m=\u001b[39m \u001b[43m_check_array_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X_array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput should be a 3D ndarray, the shape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_array\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m             )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn_env\\lib\\site-packages\\gtda\\utils\\validation.py:19\u001b[0m, in \u001b[0;36m_check_array_mod\u001b[1;34m(X, **kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Modified version of :func:`sklearn.utils.validation.check_array. When\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mkeyword parameter `force_all_finite` is set to False, NaNs are not\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03maccepted but infinity is.\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforce_all_finite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     Xnew \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(Xnew \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(Xnew) \u001b[38;5;28;01melse\u001b[39;00m Xnew\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaNs. Only finite values and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfinity are allowed when parameter \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`force_all_finite` is False.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn_env\\lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nn_env\\lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    837\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Args:\n",
    "        model_type = 'D'\n",
    "        batch_size = 32  # Reduced for performance\n",
    "        epochs = 20\n",
    "        lr = 1e-3\n",
    "        latent_dim = 20\n",
    "        cheby_order = 3\n",
    "        gamma = 0.1\n",
    "        delta = 1.0\n",
    "        run_name = 'ModelD_Final_Run'\n",
    "    \n",
    "    args = Args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6887afe-932b-47be-928f-8db3198405e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3d174-71a5-46fe-b818-579f6ab4fe59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nn_env)",
   "language": "python",
   "name": "nn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
